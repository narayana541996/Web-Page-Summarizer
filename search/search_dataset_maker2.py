# -*- coding: utf-8 -*-
"""search_dataset_maker2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1gqbvB0GX-mXTTkp4kowas2trSckndr13
"""

import sys
sys.path.insert(0,r'C:\assignments\cs733-nlp\web-page-summarizer')

from dataset_tools import *

import pandas as pd




def make_dataset(html, features, dataset_path = r'search/search_dataset.csv'):
  feature_dict = {k: [] for k in features}
  forms = []
  for form in html.findAll('form'):
    forms.append(form)

    # has_search_attribute
    feature_dict = search_attributes_with_children(form, 'search', 'has_search_attribute', feature_dict)

    # has_search_text
    feature_dict = search_text_with_children(form, 'search', 'has_search_text', feature_dict)

    # has_text_input
    feature_dict = find_input_type_number(form, 'text', 'has_text_input', feature_dict)
    
    # has_button
    feature_dict = find_button_number(form, 'has_button', feature_dict)

    # search_class
    feature_dict = find_class(form, 'search_class', feature_dict)
    
 
  # data=pd.DataFrame(feature_dict)
  # data.to_csv(dataset_path, mode= 'a', encoding= 'ISO-8859-1', header= False)
  # with open(r'search/forms.txt', mode = 'w', encoding= 'ISO-8859-1') as f:
  #   for item in forms:
  #     f.write(f'{forms.index(item)}) {item}\n')
  write_dataset(feature_dict, forms, dataset_path, r'search/forms.txt')

# page = requests.get(url)
if __name__ == '__main__':
  features = ['has_search_attribute', 'has_search_text', 'has_text_input', 'has_button', 'search_class']
  launch_make_dataset(features, make_dataset, dataset_path = r'search/search_dataset.csv')
